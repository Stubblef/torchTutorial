{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMERS AND MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of “attention” in the literature, but the one we will use here is the following: the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to “attend” more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
    "注意力机制描述了神经网络中最近的一组新层，它们在过去几年中引起了人们的广泛兴趣，特别是在序列任务中。文献中对“注意力”有很多不同的可能定义，但我们在这里使用的定义如下：注意力机制描述了（序列）元素的加权平均值，其权重是根据输入查询动态计算的，并且元素的键。那么这到底是什么意思呢？目标是对多个元素的特征进行平均。然而，我们不想对每个元素进行平均加权，而是根据它们的实际值对它们进行加权。换句话说，我们希望动态地决定我们希望比其他输入更多地“参与”哪些输入。特别是，注意力机制通常有四个部分需要我们指定：\n",
    "\n",
    "Query: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
    "查询：查询是一个特征向量，描述我们在序列中寻找什么，即我们可能想要注意什么。\n",
    "\n",
    "Keys: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
    "键：对于每个输入元素，我们有一个键，它也是一个特征向量。该特征向量粗略地描述了该元素“提供”什么，或者它何时可能很重要。键的设计应该使得我们可以根据查询来识别我们想要关注的元素。\n",
    "\n",
    "Values: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
    "值：对于每个输入元素，我们还有一个值向量。这个特征向量就是我们想要平均的向量。\n",
    "\n",
    "Score function: To rate which elements we want to pay attention to, we need to specify a score function f_{attn}. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
    "评分函数：为了对我们想要关注的元素进行评分，我们需要指定一个评分函数 f_{attn} 。得分函数将查询和键作为输入，并输出查询-键对的得分/注意力权重。它通常通过简单的相似性度量来实现，例如点积或小型 MLP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: # Enable inline plotting zh: 启用内联绘图\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# PyTorch Lightning\n",
    "import lightning as L\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.set_cmap(\"cividis\")  # Set the default colormap to \"cividis\"  zh: \"cividis\"是一种颜色映射，用于在视觉上增强数据的可视化效果\n",
    "%matplotlib inline  # Enable inline plotting  zh: 启用内联绘图\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/Transformers/\")\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
